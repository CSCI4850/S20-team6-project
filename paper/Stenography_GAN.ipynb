{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% PACKAGES INCLUDED HERE \n",
    "% DO NOT NEED TO CHANGE\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "%\\IEEEoverridecommandlockouts\n",
    "% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\n",
    "\\usepackage{cite}\n",
    "\\usepackage{amsmath,amssymb,amsfonts}\n",
    "\\usepackage{algorithmic}\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{textcomp}\n",
    "\\def\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}\\kern-.08em\n",
    "    T\\kern-.1667em\\lower.7ex\\hbox{E}\\kern-.125emX}}\n",
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% TITLE GOES HERE\n",
    "\n",
    "\\title{Stenography GAN: Cracking Stenography with Cycle Generative Adversarial Networks}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% AUTHOR NAMES GOES HERE\n",
    "\n",
    "\\author{\\IEEEauthorblockN{1\\textsuperscript{st} Nibraas Khan}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN \\\\\n",
    "nak2z@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{2\\textsuperscript{nd} Ruj Haan}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN \\\\\n",
    "gm3g@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{3\\textsuperscript{rd} George Boktor}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN \\\\\n",
    "gsb3c@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{4\\textsuperscript{th} Michael McComas}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN \\\\\n",
    "mrm8m@mtmail.mtsu.edu}\n",
    "\\and\n",
    "\\IEEEauthorblockN{5\\textsuperscript{th} Ramin Daneshi}\n",
    "\\IEEEauthorblockA{\\textit{Department of Computer Science} \\\\\n",
    "\\textit{Middle Tennessee State University}\\\\\n",
    "Murfreesboro, TN \\\\\n",
    "rd3s@mtmail.mtsu.edu}\n",
    "}\n",
    "\n",
    "\\maketitle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% ABSTRACT \n",
    "\n",
    "\\begin{abstract}\n",
    "This document is a model and instructions for \\LaTeX.\n",
    "This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, \n",
    "or Math in Paper Title or Abstract.\n",
    "\\end{abstract}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% KEYWORDS\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "component, formatting, style, styling, insert\n",
    "\\end{IEEEkeywords}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% INTRODUCTION SECTION\n",
    "\\section{Introduction}\n",
    "\n",
    "Talk about the importance of cyrpto. \n",
    "\n",
    "The sten specifically. \n",
    "\n",
    "Sten used in real life.\n",
    "\n",
    "There have crack to crack cyrpto and steno. \n",
    "\n",
    "We are also taking on the same task. Our approach is cycle gans. \n",
    "\n",
    "Before that, we need to about gans in general. \n",
    "\n",
    "History of cycle gans \n",
    "\n",
    "Pix2Pix and CNN\n",
    "\n",
    "Why will this work/or why is this worth exploring \n",
    "\n",
    "In order to understand its need to compare it to other models, and in this case autoencoders\n",
    "\n",
    "For maximum accruacy we also introduced. Bayesian optimization which is ...\n",
    "\n",
    "Start typing here \\cite{b1}."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% BACKGROUND SECTION\n",
    "\\section{Background}\n",
    "\n",
    "The details of the sten algo. Include some images. Give an example. \n",
    "\n",
    "We used cycles gans to crack this algo. That etails: \n",
    "\n",
    "Regular GANs. Include images and the math\n",
    "\n",
    "Cycle GANs. Explain it a little bit \n",
    "\n",
    "Our implemention of cycle gans ultalizes pix2pix\n",
    "\n",
    "Explain what pix2pix is. Include images and rough outlay of the math \n",
    "\n",
    "Pix2pix emplyed CNN. What they are and the math behind it. \n",
    "\n",
    "CipherGANs. Explain it a little bit\n",
    "\n",
    "Autoencoders. Explain it a little bit \n",
    "\n",
    "Baysian optimzation. Explain it a little but\n",
    "\n",
    "---\n",
    "\n",
    "Steganography.\n",
    "\n",
    "Humans have been implementing steganography one way or another since at least 440 B.C. Early Greek rulers would shave the head of a slave or prisoner of war, tattoo an image or message onto his or her scalp, wait for the prisoner's hair to regrow, obscuring the message, then send the prisoner to deliver the message. Although the type of steganographic samples we'll be evaluating in this paper is a little more 21st century, the underlying principle is still there: embedding information in such a way as to be undetectable by the naked eye. The primary characteristic of our specific Stenography algorithm involves taking two images, where one image is the Hidden Image and another image is the Cover Image, and encoding the Hidden Image into the Cover Image by taking the first X number of significant digits in each of its binary string representations, the RGB value per pixel, and appending it to the corresponding strings in the Cover Image.\n",
    "\n",
    "Generative Adversarial Networks.\n",
    "\n",
    "To begin understanding how we approached the problem of cracking the Stenography algorithm, it would be useful to begin with Generative Adversarial Networks (GANs), generative models that are able to produce high quality content (high quality in this case meaning any kind of data that closely resembles an optimal distribution specified for a given task) through adversarial learning. GANs were first introduced in 2014 by Ian J. Goodfellow and his colleagues as a way to circumvent the common pitfalls of deep generative models at the time. They introduced 'adversarial nets' that pit two networks against each other in an attempt to refine the generative capabilities of a generator network. Specifically, GANs partake in a discrimination task wherein a discriminator network is trained to classify content as real or fake and the generator network is trained to produce content real enough to fool the discriminator network. For GANs, after each training cycle, a classification error is evaluated based on how often the discriminator failed in its discrimination task, and the goal of the generator network is to maximize this classification error while the discriminator network attempts to minimize that error. Weights are adjusted according to each network's task: the generator via gradient ascent over its parameters and the discriminator via gradient descent over its parameters. [ADD CITATION HERE] As both networks become more proficient in their respective tasks, the adversarial nature of the training regime will allow the generator to produce more accurate content with respect to what the task demands.\n",
    "\n",
    "Cycle Generative Adversarial Networks. \n",
    " \n",
    "Cycle GANs demonstrate a natural progression of GANs architecture. These models are useful for image-to-image translation, commonly implemented as style transfer, wherein the architecture learns to translate one image from one domain to the style or pattern of another image from another domain. Unlike regular GANs, which implement one discriminator network and one generator network to achieve realism in one given image domain, Cycle GANs implement two or more of these networks to achieve a translation of one image domain to another. Specifically, this is achieved by training two generators and two discriminators, two distinct GAN implementations, where Model A's generator is discriminated by Model B's discriminator, and both networks are configured to their respective image domains.\n",
    "\n",
    "pix2pix.\n",
    " \n",
    "Our Cycle GANs architecture implements pix2pix, a service which learns the mapping of an input image and generates a corresponding output image. Pix2pix was developed by a Dutch broadcasting network, NPO, in an attempt to create an AI system that could analyze that could take user-created images and convert them to their life-like representaiton. Ideally, the model aims to translate one possible representation of a scene or image into another, but the native approach with Convolutional Neural Networks isn't ideal because they train to minimize the loss function, and therefore, produce blurry images. Generative Adversarial Networks circumvent this problem because they aim to classify fake (blurry or imprecise) information while also reducing the loss on the generator's part. The loss in a GANs architecture adapts to the data, and therefore, many different kinds of images can be generated with pix2pix. An example of pix2pix pattern translation is taking a series of abstract squares on a canvas and transforming them into windows on a brick building. Unlike pure Cycle GANs, which, for our implementation, is useful for translating styles or patterns from one concrete image to another, pix2pix aims to translate abstract patterns into concrete images, and can emulate that process in reverse, and can even do things like remove backgrounds from images and convert edge drawings into life-like pictures of cats! \n",
    "\n",
    "Cipher Generative Adversarial Networks.\n",
    "\n",
    "Current literature is also attempting to crack cryptography, one example of this is Cipher GANs. Related to Cycle GANs, Cipher GANs, a type of model that can understand the underlying cipher mapping of unordered data given the encrypted and unecrypted corpus, are more useful in discrete tasks such as text alignment because they more effectively address the problem of flat gradients through \"discrete variables\" and they also solve the problem of uninformative discriminiation in a GANs architecture wherein the discriminator favors a discimination criterion that is uninformative when compared to the re-discretized samples from the generator. The philosophy of Cipher GANs is particularly important to us because our problem domains are similar. Unlike Cipher GANs which aim to crack discretized data in the form of text and cipher sequences, we aim to crack embedded images by learning the steganographic alogrithm with which they're encoded.\n",
    "\n",
    "Autoencoders. \n",
    "\n",
    "To show the effectiveness of Cycle GANs we implement another machine learning algorithm to crack stegongraphy. Autoencoders are a type of unsupervised learning model that is particularly good at feature extraction by trying to approximate an identity function that produces target values similar to the input values of the network. Autoencoders, along with other unsupervised learning techniques, were discussed as early as 1987 as a proposed method for unsupervised pre-training of artifical neural networks. We're attempting to use Convolutional Autoencoders to demonstrate its similar utility to Cycle GANs for cracking Stegonagraphy due to this type of model's ability to learn embedded features in a dataset. \n",
    "\n",
    "BAYESIAN OPTIMIZATION\n",
    "\n",
    "To further improve the accuracy of our model we implement Bayesian optimization which uses Bayesian inference to improve the hyper parameters. \n",
    "\n",
    "\n",
    "\n",
    "MY CURRENT SOURCES:\n",
    "\n",
    "http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w13/Yuan_Unsupervised_Image_Super-Resolution_CVPR_2018_paper.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1611.07004.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1801.04883.pdf\n",
    "\n",
    "http://ailab.chonbuk.ac.kr/seminar_board/pds1_files/sparseAutoencoder.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1712.02950.pdf\n",
    "\n",
    "https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1404.7828.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1406.2661.pdf\n",
    "\n",
    "http://csis.pace.edu/~ctappert/srd2005/d1.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "https://aaltodoc.aalto.fi/handle/123456789/42782"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% METHODS SECTION\n",
    "\\section{Methods}\n",
    "\n",
    "Math for the cycle gans\n",
    "\n",
    "Math behind the autoencoders \n",
    "\n",
    "Math bethind the bayes opt\n",
    "\n",
    "Training CycleGAN: \n",
    "    The training and testing data\n",
    "    How we actually flow through the network \n",
    "    Bayes opt\n",
    "    \n",
    "Training Autoencoders:\n",
    "    The training and testing data\n",
    "    How we actually flow through the network\n",
    "    \n",
    "Testing protocols: \n",
    "    Testing\n",
    "    \n",
    "Different training techniques:\n",
    "    Bit size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% RESULTS SECTION\n",
    "\\section{Results}\n",
    "\n",
    "Here are the result for cycle gan: \n",
    "     Grab images from the cycle gan algo\n",
    "\n",
    "Here are the results for autoencoder:\n",
    "     Grab images from the autoencoder section \n",
    "     \n",
    "Here are the results for messing with different bit size:\n",
    "     Images from bit = 7, 6, 5, 4, 3\n",
    "\n",
    "Here are the results for cycle gan with bayes opt:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% DISCUSSION SECTION\n",
    "\\section{Discussion}\n",
    "\n",
    "Summary of the introduction \n",
    "\n",
    "If our model was successful. Why was it not successful. \n",
    "\n",
    "Extra things that we can work on. \n",
    "\n",
    "Our model presentes many fruitful avenues of research "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "% REFERENCES\n",
    "% THIS IS CREATED AUTOMATICALLY\n",
    "\\bibliographystyle{IEEEtran}\n",
    "\\bibliography{References} % change if another name is used for References file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
